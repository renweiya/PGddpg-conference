正在比较文件 round_up.py 和 ROUND_UP4.PY
***** round_up.py
            agent.spread_rewards = True if i < num_adversaries else False
            agent.size = 0.035 if agent.adversary else 0.035 #modify by laker
            ######success#agent.size = 0.035 if agent.adversary else 0.075 #modify by laker
            agent.accel = 1.0 if agent.adversary else 1.0
            agent.max_speed = 0.5 if agent.adversary else 0.5
            #######success# agent.accel = 2.0 if agent.adversary else 2.0
            #######success# agent.max_speed = 1.0 if agent.adversary else 1.3
        # add landmarks
***** ROUND_UP4.PY
            agent.spread_rewards = True if i < num_adversaries else False
            # agent.size = 0.075 if agent.adversary else 0.075 #modify by laker
            agent.size = 0.025 if agent.adversary else 0.075 #modify by laker
            #agent.size = 0.05 if agent.adversary else 0.075
            # agent.accel = 3.0 if agent.adversary else 4.0
            agent.accel = 2.0 if agent.adversary else 2.0
            agent.max_speed = 1.0 if agent.adversary else 1.3
        # add landmarks
*****

***** round_up.py
                    # print('collision...')
                    rew -= 10 #laker ori:10

***** ROUND_UP4.PY
                    # print('collision...')
                    rew -= 10

*****

***** round_up.py

        #laker
        # TODO: if bounded, then hidden this code.
        # for p in range(world.dim_p):
        #     x = abs(agent.state.p_pos[p])
        #     rew -= bound(x)

        return rew
***** ROUND_UP4.PY

        # TODO: if bounded, then hidden this code.
        for p in range(world.dim_p):
            x = abs(agent.state.p_pos[p])
            rew -= bound(x)
        return rew
*****

***** round_up.py
    def adversary_reward(self, agent, world):
        step_penalize = 0
        # step_penalize = 0
        # Adversaries are rewarded for collisions with agents
***** ROUND_UP4.PY
    def adversary_reward(self, agent, world):
        # step_penalize = -0.1
        step_penalize = 0
        # Adversaries are rewarded for collisions with agents
*****

***** round_up.py
        #         if self.is_collision(predator_, partners_, collision_level=1):
        #             shape_rew=shape_rew-0.1
        if agent.collide:
***** ROUND_UP4.PY
        #         if self.is_collision(predator_, partners_, collision_level=1):
        #             shape_rew=shape_rew-0.01

        if agent.collide:
*****

***** round_up.py
                    # more catching,more reward 
                    if dis_idx == 0 and self_collision==1:#laker no reward here
                        shape_rew=0  #0.1*collision_num*self.rewards_base
                        
***** ROUND_UP4.PY
                    # more catching,more reward 
                    if dis_idx == 0 and self_collision==1:
                        shape_rew=1#0.1*collision_num*self.rewards_base
                        
*****

***** round_up.py
            if not other.adversary:  # 瑙瀵琚椋锛娣诲堕搴
                #for sure
                other_pos.append(other.state.p_pos - agent.state.p_pos)  # 2
                other_vel.append(other.state.p_vel)
***** ROUND_UP4.PY
            if not other.adversary:  # 瑙瀵琚椋锛娣诲堕搴
                other_vel.append(other.state.p_vel)
*****

***** round_up.py
        
        return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos +  other_vel + other_pos)

***** ROUND_UP4.PY
        
        return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + other_vel)

*****

